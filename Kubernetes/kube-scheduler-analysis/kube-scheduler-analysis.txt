


============================================== k8s affinity =====================================================

https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/


============================================== k8s Topology spread constraint ========================================

https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/



------------------------------------- configuration that affect kube scheduler ---------------------------------------


The Kubernetes scheduler is responsible for assigning pods to nodes in a cluster based on various factors. There are several configuration options that can affect the behavior of the kube-scheduler. Here are some key configuration parameters:

Pod Priority and Preemption: The kube-scheduler can prioritize certain pods over others based on their priority class. You can configure priority classes for your pods, and the scheduler will take them into account when making scheduling decisions. Additionally, you can enable preemption to allow lower-priority pods to be evicted in favor of higher-priority pods.

Node Selector and Affinity: You can specify node selectors or affinity rules in your pod specifications to influence the scheduler's decision. Node selectors allow you to define constraints on which nodes the pod can be scheduled, based on labels assigned to the nodes. Affinity rules provide more fine-grained control over pod placement, allowing you to specify preferences and anti-affinities between pods and nodes.

Resource Requirements and Limits: Pods can specify their resource requirements (e.g., CPU and memory) and limits. The scheduler takes these requirements into consideration when assigning pods to nodes, ensuring that the nodes have sufficient resources to accommodate the pod's demands.

Taints and Tolerations: Nodes can be tainted to repel certain pods from being scheduled on them. Pods, in turn, can specify tolerations for specific taints, indicating that they can tolerate being scheduled on tainted nodes. This configuration can be used to influence pod placement based on specific node properties or characteristics.

Scheduling Policies and Plugins: The kube-scheduler supports various scheduling policies and plugins that can be enabled or customized to influence the scheduling decisions. For example, you can configure the inter-pod affinity/anti-affinity policy, the pod topology spread constraints, or even write custom schedulers using scheduler plugins.

Resource Scarcity and QoS: If there is a shortage of resources in the cluster, such as CPU or memory, the scheduler may make decisions based on the Quality of Service (QoS) classes of the pods. Pods with higher QoS classes (e.g., Guaranteed) are given priority over pods with lower QoS classes (e.g., Burstable or BestEffort).

Scheduling Constraints and Limits: You can set constraints and limits on the number of pods that can be scheduled on a node or in a namespace. These constraints help prevent resource overutilization and ensure fair distribution of resources among pods.

These are some of the key configuration aspects that can affect the behavior of the kube-scheduler and influence pod placement decisions in a Kubernetes cluster.





----------------------how to distributes pods with many deployments having same image. ----------------------------------------------------------------


When you have multiple deployments with the same image and you want to distribute the pods evenly across the available nodes in your Kubernetes cluster, you can use a combination of labels, node selectors, and affinity rules. Here's a step-by-step approach:

Add Labels to Nodes: Start by adding labels to the nodes in your cluster. For example, you can add a label like node-type=worker to indicate that a node is a worker node.

Create Deployments: Create multiple deployments with the same image that you want to distribute. Make sure to define unique labels for each deployment.

Define Node Selector: In the pod template specification of each deployment, add a node selector that matches the label(s) you applied to the nodes. For example, if you labeled your nodes with node-type=worker, add the following to the pod template specification:

yaml
Copy code
spec:
  nodeSelector:
    node-type: worker
Define Pod Affinity/Anti-Affinity: To ensure that the pods are evenly distributed across nodes, you can use pod affinity or anti-affinity rules. Add the following to the pod template specification of each deployment:

yaml
Copy code
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - <deployment-label>
            topologyKey: "kubernetes.io/hostname"
Replace <deployment-label> with the unique label you defined for each deployment. This rule will ensure that the scheduler tries to avoid scheduling pods with the same label on the same node.

Apply Deployments: Apply the deployment manifests to your Kubernetes cluster using kubectl apply or any other deployment method you prefer.

With these configurations in place, the kube-scheduler will take into account the node selectors and affinity rules when making scheduling decisions. It will distribute the pods with the same image across available nodes that match the node selector, and the affinity rules will help in spreading the pods evenly across different nodes.

Note: Keep in mind that the actual distribution of pods may also depend on factors such as resource availability, pod priorities, and other scheduling configurations in your cluster.



----------------------------------------- global minimum calculation in maxskew  ----------------------------------

*** maxskew is not always gives the desired result because, achieving the global minimum may not always be possible due to various constraints such as node capacity, resource requirements, and other scheduling considerations. Kubernetes will do its best to balance the pods based on the defined constraints and maxSkew value

*** Global minimum : The "global minimum" in this context refers to the ideal distribution that Kubernetes aims to achieve, where the difference in the number of pods on each node is minimized. the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains. 
       e.g 4 pods in 4 nodes, global minimum is 1. 



If you're referring to a scenario where maxSkew is used in conjunction with calculating a "global minimum" in Kubernetes, it likely relates to the concept of pod anti-affinity and achieving workload distribution across nodes.

In Kubernetes, maxSkew is a parameter used in pod anti-affinity rules to define the maximum allowed imbalance or skew in the distribution of pods across nodes. It is used to ensure that pods of a particular type or with specific labels are spread out evenly across the available nodes.

When maxSkew is set, Kubernetes attempts to schedule pods in a way that minimizes the difference in the number of pods on each node. The "global minimum" in this context refers to the ideal distribution that Kubernetes aims to achieve, where the difference in the number of pods on each node is minimized.

For example, let's say you have a scenario where you have three nodes and want to distribute a set of pods across them. You define a maxSkew value of 1, indicating that the maximum allowed difference in the number of pods on any two nodes is 1.

Kubernetes will try to schedule the pods in a way that minimizes the difference in the number of pods across the nodes. Ideally, it would distribute the pods in a way that achieves the global minimum, where the difference in the number of pods on each node is as small as possible.

However, achieving the global minimum may not always be possible due to various constraints such as node capacity, resource requirements, and other scheduling considerations. Kubernetes will do its best to balance the pods based on the defined constraints and maxSkew value.

It's important to note that the specific implementation and behavior may vary depending on the Kubernetes version and the configuration of your cluster.




requiredDuringSchedulingIgnoredDuringExecution - vs - preferredDuringSchedulingIgnoredDuringExecution
=================================================================================================================

requiredDuringSchedulingIgnoredDuringExecution : The scheduler can't schedule the Pod unless the rule is met. This functions like nodeSelector , but with a more expressive syntax. 
preferredDuringSchedulingIgnoredDuringExecution : The scheduler tries to find a node that meets the rule



================================ topologySpreadConstraints  and affinity  precendence =============================

Summary:
    *** In terms of precedence, affinity rules take precedence over topologySpreadConstraints. This means that if there is a conflict between an affinity rule and a topologySpreadConstraint, the affinity rule will be prioritized. Kubernetes will first try to satisfy the affinity requirements and then consider the topologySpreadConstraints to distribute the pods across the desired topological domains.



In Kubernetes, both topologySpreadConstraints and affinity rules can be used to influence pod scheduling decisions. However, they serve different purposes and operate at different levels of precedence.

Affinity rules, such as nodeAffinity or podAffinity, are used to express preferences or requirements for pod placement based on node labels or other pod labels. They allow you to define rules like "place this pod on a node with a specific label" or "place this pod on a node that already has pods with specific labels." Affinity rules have a higher precedence than topologySpreadConstraints.

On the other hand, topologySpreadConstraints are used to control the spread of pods across different topology domains, such as nodes, racks, or availability zones, to achieve better fault tolerance and high availability. They ensure that pods of a specific label or type are distributed across different topological domains based on a defined spread strategy. topologySpreadConstraints are evaluated after affinity rules.

In terms of precedence, affinity rules take precedence over topologySpreadConstraints. This means that if there is a conflict between an affinity rule and a topologySpreadConstraint, the affinity rule will be prioritized. Kubernetes will first try to satisfy the affinity requirements and then consider the topologySpreadConstraints to distribute the pods across the desired topological domains.

It's worth noting that even though affinity rules have higher precedence, they do not guarantee pod placement. Scheduling decisions are also influenced by other factors such as resource availability, node constraints, and other scheduling considerations.



